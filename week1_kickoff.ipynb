{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     67\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs it going to rain tomorrow?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 68\u001b[0m prediction, confidence \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 52\u001b[0m, in \u001b[0;36mclassify_text\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m     49\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlogprobs\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtop_logprobs\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Convert log probabilities to probabilities\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m())))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Normalize the probabilities\u001b[39;00m\n\u001b[1;32m     55\u001b[0m probability_distribution \u001b[38;5;241m=\u001b[39m probabilities \u001b[38;5;241m/\u001b[39m probabilities\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "# Your mathematical functions\n",
    "def compute_entropy(probability_distribution): # shannon entropy\n",
    "    return -np.sum(probability_distribution * np.log(probability_distribution + 1e-12)) # 정보를 인코딩할 때의 정보량이 아니라, 정보의 불확실성을 측정하는 것이라는 점에서 차이가 있음.\n",
    "\n",
    "def compute_conditional_entropy(joint_distribution, marginal_distribution):\n",
    "    return -np.sum(joint_distribution * np.log((joint_distribution / (marginal_distribution + 1e-12)) + 1e-12))\n",
    "\n",
    "def compute_mutual_information(joint_distribution, marginal_distribution):\n",
    "    return compute_entropy(marginal_distribution) - compute_conditional_entropy(joint_distribution, marginal_distribution)\n",
    "\n",
    "def compute_information_gain(joint_distribution, marginal_distribution):\n",
    "    return compute_entropy(marginal_distribution) - compute_conditional_entropy(joint_distribution, marginal_distribution)\n",
    "\n",
    "def compute_relative_entropy(p, q):\n",
    "    return np.sum(p * np.log((p / (q + 1e-12)) + 1e-12))\n",
    "\n",
    "def compute_kl_divergence(p, q):\n",
    "    return np.sum(p * np.log((p / (q + 1e-12)) + 1e-12))\n",
    "\n",
    "def compute_jensen_shannon_divergence(p, q):\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * compute_kl_divergence(p, m) + 0.5 * compute_kl_divergence(q, m)\n",
    "\n",
    "def compute_chi_square_statistic(observed, expected):\n",
    "    return np.sum(((observed - expected) ** 2) / (expected + 1e-12))\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Classification function\n",
    "def classify_text(input_text):\n",
    "    messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        logprobs=True,\n",
    "        max_tokens=1,\n",
    "        top_logprobs=3\n",
    "    )\n",
    "    \n",
    "    # Extract the model's prediction\n",
    "    prediction = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract the log probabilities\n",
    "    logprobs = response.choices[0].logprobs.content[0].top_logprobs\n",
    "    \n",
    "    # Convert log probabilities to probabilities\n",
    "    probabilities = np.exp(np.array(list(logprobs.values())))\n",
    "    \n",
    "    # Normalize the probabilities\n",
    "    probability_distribution = probabilities / probabilities.sum()\n",
    "    \n",
    "    # Compute entropy\n",
    "    entropy = compute_entropy(probability_distribution)\n",
    "    \n",
    "    # Compute confidence (lower entropy means higher confidence)\n",
    "    max_entropy = np.log(len(probability_distribution))\n",
    "    confidence = 1 - (entropy / max_entropy)\n",
    "    \n",
    "    return prediction, confidence\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Is it going to rain tomorrow?\"\n",
    "prediction, confidence = classify_text(input_text)\n",
    "\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Confidence: {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\"Analyze the following text and determine if it is a positive or negative review. Answer with only one word, considering the nuanced emotional impact and potential long-term effects on the viewer: 'The film's intricate narrative weaved a tapestry of conflicting emotions, challenging societal norms while simultaneously reinforcing traditional values, leaving the audience in a state of cognitive dissonance long after the credits rolled.'\"\"\"}\n",
    "    ],\n",
    "    logprobs=True,\n",
    "    max_tokens=1,\n",
    "    top_logprobs=5,\n",
    "    temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Choice(finish_reason='length', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='The', bytes=[84, 104, 101], logprob=-3.1281633e-07, top_logprobs=[TopLogprob(token='The', bytes=[84, 104, 101], logprob=-3.1281633e-07), TopLogprob(token='Paris', bytes=[80, 97, 114, 105, 115], logprob=-15.75), TopLogprob(token=' The', bytes=[32, 84, 104, 101], logprob=-16.875), TopLogprob(token='France', bytes=[70, 114, 97, 110, 99, 101], logprob=-20.625), TopLogprob(token='As', bytes=[65, 115], logprob=-22.125)])], refusal=None), message=ChatCompletionMessage(content='The', role='assistant', function_call=None, tool_calls=None, refusal=None))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='The', role='assistant', function_call=None, tool_calls=None, refusal=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='The', bytes=[84, 104, 101], logprob=-6.704273e-07, top_logprobs=[TopLogprob(token='The', bytes=[84, 104, 101], logprob=-6.704273e-07), TopLogprob(token='Paris', bytes=[80, 97, 114, 105, 115], logprob=-14.375001), TopLogprob(token=' The', bytes=[32, 84, 104, 101], logprob=-18.0)])], refusal=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionTokenLogprob(token='The', bytes=[84, 104, 101], logprob=-6.704273e-07, top_logprobs=[TopLogprob(token='The', bytes=[84, 104, 101], logprob=-6.704273e-07), TopLogprob(token='Paris', bytes=[80, 97, 114, 105, 115], logprob=-14.375001), TopLogprob(token=' The', bytes=[32, 84, 104, 101], logprob=-18.0)])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].logprobs.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TopLogprob(token='The', bytes=[84, 104, 101], logprob=-6.704273e-07),\n",
       " TopLogprob(token='Paris', bytes=[80, 97, 114, 105, 115], logprob=-14.375001),\n",
       " TopLogprob(token=' The', bytes=[32, 84, 104, 101], logprob=-18.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].logprobs.content[0].top_logprobs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "-6.704273e-07\n",
      "0.9999993295729247\n",
      "Paris\n",
      "-14.375001\n",
      "5.715002021461846e-07\n",
      " The\n",
      "-18.0\n",
      "1.522997974471263e-08\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for content in response.choices[0].logprobs.content[0].top_logprobs:\n",
    "    print(content.token)\n",
    "    print(content.logprob)\n",
    "    print(np.exp(content.logprob))\n",
    "    i += 1\n",
    "    \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AF1CEbBCDDrW8RkTNP9UwFRiduH9e', choices=[Choice(finish_reason='length', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='The', bytes=[84, 104, 101], logprob=-6.704273e-07, top_logprobs=[TopLogprob(token='The', bytes=[84, 104, 101], logprob=-6.704273e-07), TopLogprob(token='Paris', bytes=[80, 97, 114, 105, 115], logprob=-14.375001), TopLogprob(token=' The', bytes=[32, 84, 104, 101], logprob=-18.0)])], refusal=None), message=ChatCompletionMessage(content='The', role='assistant', function_call=None, tool_calls=None, refusal=None))], created=1728142882, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_143bb8492c', usage=CompletionUsage(completion_tokens=1, prompt_tokens=24, total_tokens=25, prompt_tokens_details={'cached_tokens': 0}, completion_tokens_details={'reasoning_tokens': 0}))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty-printed response:\n",
      "{'choices': [{'finish_reason': 'length',\n",
      "              'index': 0,\n",
      "              'logprobs': {'content': [{'logprob': -0.20231643,\n",
      "                                        'token': 'Positive',\n",
      "                                        'top_logprobs': [{'logprob': -0.20231643,\n",
      "                                                          'token': 'Positive'},\n",
      "                                                         {'logprob': -1.7023164,\n",
      "                                                          'token': 'Negative'},\n",
      "                                                         {'logprob': -7.0773163,\n",
      "                                                          'token': 'Neutral'},\n",
      "                                                         {'logprob': -10.327316, 'token': 'Mixed'},\n",
      "                                                         {'logprob': -10.827316, 'token': '\"'}]}]},\n",
      "              'message': {'content': 'Positive', 'role': 'assistant'}}],\n",
      " 'created': 1728181562,\n",
      " 'id': 'chatcmpl-AFBG6bSs0R4bFVTLxbGUeP7Bx6vww',\n",
      " 'model': 'gpt-4o-2024-08-06',\n",
      " 'object': 'chat.completion',\n",
      " 'system_fingerprint': 'fp_2f406b9113',\n",
      " 'usage': {'completion_tokens': 1, 'prompt_tokens': 93, 'total_tokens': 94}}\n",
      "Response saved to 'response.json'\n",
      "0.8168364154968915\n",
      "0.18226084567271605\n",
      "0.0008440352490189136\n",
      "3.272680798079951e-05\n",
      "1.9849812434883006e-05\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "from pprint import pprint\n",
    "import numpy as np \n",
    "\n",
    "# Convert the response object to a dictionary\n",
    "response_dict = {\n",
    "    \"id\": response.id,\n",
    "    \"choices\": [{\n",
    "        \"finish_reason\": choice.finish_reason,\n",
    "        \"index\": choice.index,\n",
    "        \"logprobs\": {\n",
    "            \"content\": [{\n",
    "                \"token\": token.token,\n",
    "                \"logprob\": token.logprob,\n",
    "                \"top_logprobs\": [{\n",
    "                    \"token\": top.token,\n",
    "                    \"logprob\": top.logprob\n",
    "                } for top in token.top_logprobs]\n",
    "            } for token in choice.logprobs.content]\n",
    "        },\n",
    "        \"message\": {\n",
    "            \"content\": choice.message.content,\n",
    "            \"role\": choice.message.role\n",
    "        }\n",
    "    } for choice in response.choices],\n",
    "    \"created\": response.created,\n",
    "    \"model\": response.model,\n",
    "    \"object\": response.object,\n",
    "    \"system_fingerprint\": response.system_fingerprint,\n",
    "    \"usage\": {\n",
    "        \"completion_tokens\": response.usage.completion_tokens,\n",
    "        \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "        \"total_tokens\": response.usage.total_tokens\n",
    "    }\n",
    "}\n",
    "\n",
    "# Pretty print the response dictionary\n",
    "print(\"Pretty-printed response:\")\n",
    "pprint(response_dict, width=100, compact=False)\n",
    "\n",
    "# Optionally, you can also save the response to a JSON file\n",
    "with open('response.json', 'w') as f:\n",
    "    json.dump(response_dict, f, indent=4)\n",
    "print(\"Response saved to 'response.json'\")\n",
    "\n",
    "for logprob in response.choices[0].logprobs.content[0].top_logprobs:\n",
    "    print(np.exp(logprob.logprob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AF1CEbBCDDrW8RkTNP9UwFRiduH9e', choices=[Choice(finish_reason='length', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='The', bytes=[84, 104, 101], logprob=-6.704273e-07, top_logprobs=[TopLogprob(token='The', bytes=[84, 104, 101], logprob=-6.704273e-07), TopLogprob(token='Paris', bytes=[80, 97, 114, 105, 115], logprob=-14.375001), TopLogprob(token=' The', bytes=[32, 84, 104, 101], logprob=-18.0)])], refusal=None), message=ChatCompletionMessage(content='The', role='assistant', function_call=None, tool_calls=None, refusal=None))], created=1728142882, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_143bb8492c', usage=CompletionUsage(completion_tokens=1, prompt_tokens=24, total_tokens=25, prompt_tokens_details={'cached_tokens': 0}, completion_tokens_details={'reasoning_tokens': 0}))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "[0.99, 0.01, 0, 0, 0]\n",
    "\n",
    "\n",
    "확률 10개로 구성된 확률분포가 있을 때 \n",
    "uniform -> uncertainty가 가장 높음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p = [p1, p2, p3, p4]\n",
    "p1 + p2 + p3 + p4 = 1\n",
    "\n",
    "\n",
    "entropy = -sum(p * log(p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_logprobs = response.choices[0].logprobs.content[0].top_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearProb = [np.exp(logprob.logprob) for logprob in top_logprobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8168364154968915,\n",
       " 0.18226084567271605,\n",
       " 0.0008440352490189136,\n",
       " 3.272680798079951e-05,\n",
       " 1.9849812434883006e-05]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model confidence: 0.7005\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_confidence(logprobs):\n",
    "    # Extract the log probabilities\n",
    "    log_probs = np.array([lp.logprob for lp in logprobs])\n",
    "    \n",
    "    # Convert log probabilities to probabilities\n",
    "    probs = np.exp(log_probs)\n",
    "    \n",
    "    # Normalize the probabilities\n",
    "    probs = probs / np.sum(probs)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = -np.sum(probs * np.log2(probs))\n",
    "    \n",
    "    # Calculate confidence (1 - normalized entropy)\n",
    "    max_entropy = np.log2(len(probs))  # maximum possible entropy\n",
    "    normalized_entropy = entropy / max_entropy\n",
    "    confidence = 1 - normalized_entropy\n",
    "    \n",
    "    return confidence\n",
    "\n",
    "# Extract logprobs from the response\n",
    "logprobs = response.choices[0].logprobs.content[0].top_logprobs\n",
    "\n",
    "# Calculate and print the confidence\n",
    "confidence = calculate_confidence(logprobs)\n",
    "print(f\"Model confidence: {confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TopLogprob(token='The', bytes=[84, 104, 101], logprob=-6.704273e-07),\n",
       " TopLogprob(token='Paris', bytes=[80, 97, 114, 105, 115], logprob=-14.375001),\n",
       " TopLogprob(token=' The', bytes=[32, 84, 104, 101], logprob=-18.0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "-6.704273e-07\n",
      "0.9999993295729247\n",
      "Paris\n",
      "-14.375001\n",
      "5.715002021461846e-07\n",
      " The\n",
      "-18.0\n",
      "1.522997974471263e-08\n"
     ]
    }
   ],
   "source": [
    "for logprob in logprobs:\n",
    "    print(logprob.token)\n",
    "    print(logprob.logprob)\n",
    "    print(np.exp(logprob.logprob))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Other Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Maximum Softmax Probability -> 가장 확률 높은걸 취한다 \n",
    "2. Prediction Margin -> 가장 확률 높은거랑 두번째로 확률 높은거의 차이를 취한다. \n",
    "3. Monte Carlo Dropout -> 모델을 여러번 실행하고 그 중 가장 확률 높은거를 취한다. -> 각각의 응답 생성시에 그 자신감을 측정하겠다. (이 컨셉은 pipeline evaluation) -> prompt tuning + temperature sampling\n",
    "4. Bayesian Neural Networks -> 모델을 여러번 실행하고 그 중 가장 확률 높은거를 취한다. (finetuning)\n",
    "5. Information Theory-based Confidence -> 확률분포의 불확실성을 측정한다. (entropy)\n",
    "6. Ensemble Methods -> 모델을 여러가지를 동시에 실행하고 종합하여 결론을 내림\n",
    "7. Gradient Magnitude -> 모델의 기울기를 측정한다. (이거는 API상으로 불가능)\n",
    "8. Confidence in the Confidence (모델 응답의 확률 분포의 자신감)\n",
    "9. Self-Supervised Confidence Estimation -> 모델의 자신감을 측정한다. (이거는 API상으로 불가능)\n",
    "10. Adversarial Examples\n",
    "11. Human-in-the-Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Positive\n",
      "Confidence (MSP): 0.88\n",
      "Probability Distribution: [8.80430608e-01 1.19153328e-01 3.79238925e-04 3.52747149e-05\n",
      " 1.54986281e-06]\n"
     ]
    }
   ],
   "source": [
    "# Maximum Softmax Probability\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "client = OpenAI()\n",
    "\n",
    "def classify_with_msp(input_text):\n",
    "    messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_tokens=1,\n",
    "        logprobs=True,\n",
    "        top_logprobs=5,\n",
    "        temperature=0,  # Deterministic output\n",
    "    )\n",
    "    \n",
    "    # Extract the model's prediction\n",
    "    prediction = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract log probabilities\n",
    "    logprobs = [logprob.logprob for logprob in response.choices[0].logprobs.content[0].top_logprobs]\n",
    "    \n",
    "    # Convert log probabilities to probabilities\n",
    "    probabilities = np.exp(np.array(logprobs))\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    probability_distribution = probabilities / probabilities.sum()\n",
    "    \n",
    "    # Maximum Softmax Probability\n",
    "    confidence = np.max(probability_distribution)\n",
    "    \n",
    "    return prediction, confidence, probability_distribution\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Analyze the following text and determine if it is a positive or negative review. Answer with only one word, considering the nuanced emotional impact and potential long-term effects on the viewer: 'The film's intricate narrative weaved a tapestry of conflicting emotions, challenging societal norms while simultaneously reinforcing traditional values, leaving the audience in a state of cognitive dissonance long after the credits rolled.'\"\n",
    "prediction, confidence, probability_distribution = classify_with_msp(input_text)\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Confidence (MSP): {confidence:.2f}\")\n",
    "print(f\"Probability Distribution: {probability_distribution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopLogprob(token='Positive', bytes=[80, 111, 115, 105, 116, 105, 118, 101], logprob=-0.20231643)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].logprobs.content[0].top_logprobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Positive\n",
      "Confidence (Margin): 0.36\n"
     ]
    }
   ],
   "source": [
    "# Prediction Margin\n",
    "\n",
    "def classify_with_margin(input_text):\n",
    "    messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_tokens=1,\n",
    "        logprobs=True,\n",
    "        top_logprobs=5,\n",
    "        temperature=0,\n",
    "        n=1\n",
    "    )\n",
    "    \n",
    "    # Extract the model's prediction\n",
    "    prediction = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract log probabilities\n",
    "    logprobs = [logprob.logprob for logprob in response.choices[0].logprobs.content[0].top_logprobs]\n",
    "    \n",
    "    # Convert log probabilities to probabilities\n",
    "    probabilities = np.exp(np.array(logprobs))\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    probability_distribution = probabilities / probabilities.sum()\n",
    "    \n",
    "    # Sort probabilities\n",
    "    sorted_probs = np.sort(probability_distribution)\n",
    "    \n",
    "    # Prediction Margin\n",
    "    if len(sorted_probs) > 1:\n",
    "        margin = sorted_probs[-1] - sorted_probs[-2]\n",
    "    else:\n",
    "        margin = sorted_probs[-1]\n",
    "    \n",
    "    confidence = margin\n",
    "    \n",
    "    return prediction, confidence\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Analyze the following text and determine if it is a positive or negative review. Answer with only one word, considering the nuanced emotional impact and potential long-term effects on the viewer: 'The film's intricate narrative weaved a tapestry of conflicting emotions, challenging societal norms while simultaneously reinforcing traditional values, leaving the audience in a state of cognitive dissonance long after the credits rolled.'\"\n",
    "prediction, confidence = classify_with_margin(input_text)\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Confidence (Margin): {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Positive\n",
      "Confidence (Entropy): 0.58\n"
     ]
    }
   ],
   "source": [
    "# Information Theory-based Confidence\n",
    "\n",
    "def compute_entropy(probabilities):\n",
    "    return -np.sum(probabilities * np.log(probabilities + 1e-12))\n",
    "\n",
    "def classify_with_entropy(input_text):\n",
    "    messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_tokens=1,\n",
    "        logprobs=True,\n",
    "        top_logprobs=5,\n",
    "        temperature=0,\n",
    "    )\n",
    "    \n",
    "    # Extract prediction\n",
    "    prediction = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract log probabilities\n",
    "    logprobs = [logprob.logprob for logprob in response.choices[0].logprobs.content[0].top_logprobs]\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probabilities = np.exp(np.array(logprobs))\n",
    "    probabilities /= probabilities.sum()\n",
    "    \n",
    "    # Compute entropy\n",
    "    entropy = compute_entropy(probabilities)\n",
    "    \n",
    "    # Normalize entropy to get confidence between 0 and 1\n",
    "    max_entropy = np.log(len(probabilities))\n",
    "    confidence = 1 - (entropy / max_entropy)\n",
    "    \n",
    "    return prediction, confidence\n",
    "\n",
    "# Example usage\n",
    "prediction, confidence = classify_with_entropy(input_text)\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Confidence (Entropy): {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Positive\n",
      "Confidence (Frequency): 0.90\n",
      "Confidence (Entropy-based): 0.53\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def classify_with_temperature_sampling(input_text, num_samples=300, temperature=0.7):\n",
    "    messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages,\n",
    "            max_tokens=1,\n",
    "            temperature=temperature,\n",
    "            logprobs=True,\n",
    "            top_logprobs=5,\n",
    "        )\n",
    "        \n",
    "        # for i, logprob in enumerate(response.choices[0].logprobs.content[0].top_logprobs):\n",
    "        #     predictions.append(logprob.token)\n",
    "        prediction = response.choices[0].message.content.strip()\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # Calculate prediction frequencies\n",
    "    unique_predictions, counts = np.unique(predictions, return_counts=True)\n",
    "    probabilities = counts / num_samples\n",
    "\n",
    "    # Confidence is the probability of the most frequent prediction\n",
    "    max_index = np.argmax(probabilities)\n",
    "    final_prediction = unique_predictions[max_index]\n",
    "    confidence = probabilities[max_index]\n",
    "    \n",
    "    # Entropy of the prediction distribution\n",
    "    entropy = -np.sum(probabilities * np.log(probabilities + 1e-12))\n",
    "    max_entropy = np.log(len(unique_predictions))\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if max_entropy == 0:\n",
    "        normalized_entropy = 0\n",
    "    else:\n",
    "        normalized_entropy = entropy / max_entropy\n",
    "    \n",
    "    confidence_from_entropy = 1 - normalized_entropy\n",
    "\n",
    "    return final_prediction, confidence, confidence_from_entropy\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Analyze the following text and determine if it is a positive or negative review. Answer with only one word, considering the nuanced emotional impact and potential long-term effects on the viewer: 'The film's intricate narrative weaved a tapestry of conflicting emotions, challenging societal norms while simultaneously reinforcing traditional values, leaving the audience in a state of cognitive dissonance long after the credits rolled.'\"\n",
    "prediction, confidence, entropy_confidence = classify_with_temperature_sampling(input_text)\n",
    "\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Confidence (Frequency): {confidence:.2f}\")\n",
    "print(f\"Confidence (Entropy-based): {entropy_confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "Negative\n",
      "Positive\n",
      "Positive\n",
      "Negative\n",
      "Positive\n",
      "Positive\n",
      "Neutral\n",
      "Positive\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Analyze the following text and determine if it is a positive or negative review. Answer with only one word, considering the nuanced emotional impact and potential long-term effects on the viewer: 'The film's intricate narrative weaved a tapestry of conflicting emotions, challenging societal norms while simultaneously reinforcing traditional values, leaving the audience in a state of cognitive dissonance long after the credits rolled.'\"}\n",
    "        ],\n",
    "        logprobs=True,\n",
    "        max_tokens=1,\n",
    "        top_logprobs=5,\n",
    "    )\n",
    "    print(response.choices[0].message.content.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1차 검증 -> [classification]  \n",
    "비교하는 분포를 다르게 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Positive\n",
      "Confidence (KL Divergence): 0.61\n",
      "\n",
      "Probability distribution:\n",
      "Positive: 0.6789\n",
      "Negative: 0.3207\n",
      "Neutral: 0.0004\n",
      "Mixed: 0.0001\n",
      "Complex: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()  # Assumes API key is set in environment variable\n",
    "\n",
    "def compute_kl_divergence(p, q):\n",
    "    return np.sum(p * np.log((p + 1e-12) / (q + 1e-12)))\n",
    "\n",
    "def classify_with_kl_divergence(input_text):\n",
    "    messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_tokens=1,\n",
    "        logprobs=True,\n",
    "        temperature=0,\n",
    "        top_logprobs=5,\n",
    "    )\n",
    "    \n",
    "    # Extract prediction\n",
    "    prediction = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract log probabilities\n",
    "    logprobs = response.choices[0].logprobs.content[0].top_logprobs\n",
    "    tokens = [item.token for item in logprobs]\n",
    "    logprob_values = np.array([item.logprob for item in logprobs])\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probabilities = np.exp(logprob_values)\n",
    "    probabilities /= probabilities.sum()\n",
    "    \n",
    "    # Reference distribution (maximum entropy distribution)\n",
    "    max_entropy_probs = np.ones_like(probabilities) / len(probabilities)\n",
    "    \n",
    "    # Compute KL Divergence\n",
    "    kl_div = compute_kl_divergence(probabilities, max_entropy_probs)\n",
    "    \n",
    "    # Normalize KL Divergence to [0,1] for confidence\n",
    "    max_kl_div = np.log(len(probabilities))\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if max_kl_div == 0:\n",
    "        confidence = 0\n",
    "    else:\n",
    "        confidence = kl_div / max_kl_div\n",
    "    \n",
    "    return prediction, confidence, probabilities, tokens\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Analyze the following text and determine if it is a positive or negative review. Answer with only one word, considering the nuanced emotional impact and potential long-term effects on the viewer: 'The film's intricate narrative weaved a tapestry of conflicting emotions, challenging societal norms while simultaneously reinforcing traditional values, leaving the audience in a state of cognitive dissonance long after the credits rolled.'\"\n",
    "prediction, confidence, probabilities, tokens = classify_with_kl_divergence(input_text)\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Confidence (KL Divergence): {confidence:.2f}\")\n",
    "print(\"\\nProbability distribution:\")\n",
    "for token, prob in zip(tokens, probabilities):\n",
    "    print(f\"{token}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Positive\n",
      "Confidence (Information Gain): 0.58\n"
     ]
    }
   ],
   "source": [
    "def compute_information_gain(probabilities):\n",
    "    max_entropy = np.log(len(probabilities))\n",
    "    entropy = -np.sum(probabilities * np.log(probabilities + 1e-12))\n",
    "    ig = max_entropy - entropy\n",
    "    return ig\n",
    "\n",
    "def classify_with_information_gain(input_text):\n",
    "    messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_tokens=1,\n",
    "        logprobs=True,\n",
    "        temperature=0,\n",
    "        top_logprobs=5\n",
    "    )\n",
    "    \n",
    "    prediction = response.choices[0].message.content.strip()\n",
    "    \n",
    "    logprobs = response.choices[0].logprobs.content[0].top_logprobs\n",
    "    logprob_values = np.array([item.logprob for item in logprobs])\n",
    "    probabilities = np.exp(logprob_values)\n",
    "    probabilities /= probabilities.sum()\n",
    "    \n",
    "    # Compute Information Gain\n",
    "    ig = compute_information_gain(probabilities)\n",
    "    \n",
    "    # Normalize IG to [0,1]\n",
    "    max_ig = np.log(len(probabilities))\n",
    "    confidence = ig / max_ig\n",
    "    \n",
    "    return prediction, confidence\n",
    "\n",
    "# Example usage\n",
    "prediction, confidence = classify_with_information_gain(input_text)\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Confidence (Information Gain): {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이쯤 생길만한 의문이 좀 있을 것 같아서...\n",
    "\n",
    "Why Use These Information Theory Measures?   \n",
    "\n",
    "\n",
    "KL Divergence: Measures how much the model's predicted distribution diverges from a uniform distribution. A higher divergence indicates that the model's predictions are concentrated on specific tokens, suggesting higher confidence.\n",
    "\n",
    "Jensen-Shannon Divergence: A symmetric and smoothed version of KL Divergence, providing a normalized measure of similarity between distributions.\n",
    "\n",
    "Information Gain: Reflects the reduction in uncertainty after observing the model's output. Higher information gain implies that the model's output provides significant information about the diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE USE CASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Develop a language model pipeline that provides diagnostic suggestions based on patient symptoms, with a focus on ensuring high confidence in the model's responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(probabilities):\n",
    "    return -np.sum(probabilities * np.log(probabilities + 1e-12))\n",
    "\n",
    "def compute_kl_divergence(p, q):\n",
    "    return np.sum(p * np.log((p + 1e-12) / (q + 1e-12)))\n",
    "\n",
    "def compute_jensen_shannon_divergence(p, q):\n",
    "    m = 0.5 * (p + q)\n",
    "    jsd = 0.5 * compute_kl_divergence(p, m) + 0.5 * compute_kl_divergence(q, m)\n",
    "    return jsd\n",
    "\n",
    "def compute_information_gain(probabilities):\n",
    "    max_entropy = np.log(len(probabilities))\n",
    "    entropy = compute_entropy(probabilities)\n",
    "    ig = max_entropy - entropy\n",
    "    return ig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_divergence_confidence(probabilities):\n",
    "    uniform_probabilities = np.ones_like(probabilities) / len(probabilities)\n",
    "    kl_div = compute_kl_divergence(probabilities, uniform_probabilities)\n",
    "    max_kl_div = np.log(len(probabilities))\n",
    "    confidence = kl_div / max_kl_div\n",
    "    return confidence\n",
    "\n",
    "def compute_jsd_confidence(probabilities):\n",
    "    uniform_probabilities = np.ones_like(probabilities) / len(probabilities)\n",
    "    jsd = compute_jensen_shannon_divergence(probabilities, uniform_probabilities)\n",
    "    max_jsd = np.log(2)\n",
    "    confidence = jsd / max_jsd\n",
    "    return confidence\n",
    "\n",
    "def compute_information_gain_confidence(probabilities):\n",
    "    ig = compute_information_gain(probabilities)\n",
    "    max_ig = np.log(len(probabilities))\n",
    "    confidence = ig / max_ig\n",
    "    return confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_with_confidence(symptoms_text):\n",
    "    messages = [{\"role\": \"user\", \"content\": symptoms_text}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_tokens=1,  # Allow for detailed responses\n",
    "        temperature=0,  # Deterministic output\n",
    "        logprobs=True,\n",
    "        top_logprobs=5\n",
    "    )\n",
    "    \n",
    "    # Extract the model's response\n",
    "    diagnosis = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract log probabilities of the first token (as an approximation)\n",
    "    logprobs = response.choices[0].logprobs.content[0].top_logprobs\n",
    "    tokens = [logprob.token for logprob in logprobs]\n",
    "    logprob_values = np.array([logprob.logprob for logprob in logprobs])\n",
    "    \n",
    "    # Convert log probabilities to probabilities\n",
    "    probabilities = np.exp(logprob_values)\n",
    "    probabilities /= probabilities.sum()\n",
    "    \n",
    "    # Compute confidence measures\n",
    "    kl_confidence = compute_kl_divergence_confidence(probabilities)\n",
    "    jsd_confidence = compute_jsd_confidence(probabilities)\n",
    "    ig_confidence = compute_information_gain_confidence(probabilities)\n",
    "    \n",
    "    # Combine confidence scores (weighted sum)\n",
    "    combined_confidence = (0.4 * kl_confidence) + (0.3 * jsd_confidence) + (0.3 * ig_confidence)\n",
    "    \n",
    "    return diagnosis, combined_confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is not confident enough in its diagnosis.\n",
      "Confidence: 0.55\n",
      "Recommendation: Consult a medical professional for further evaluation.\n"
     ]
    }
   ],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.75  # Adjust based on validation results\n",
    "\n",
    "# Example patient symptoms\n",
    "symptoms_text = \"\"\"\n",
    "Patient presents with the following symptoms:\n",
    "1. Persistent chest pain\n",
    "2. Severe shortness of breath\n",
    "3. High fever\n",
    "4. Productive cough with greenish or yellow sputum\n",
    "Additional notes:\n",
    "- Chest pain worsens when breathing deeply or coughing\n",
    "- Shortness of breath even at rest\n",
    "- Fatigue and weakness\n",
    "- Rapid breathing and increased heart rate\n",
    "- Possible bluish tint to lips or fingernails (cyanosis)\n",
    "\n",
    "Medical history suggests patient is suffering from pneumonia.\n",
    "\"\"\"\n",
    "\n",
    "diagnosis, confidence = diagnose_with_confidence(symptoms_text)\n",
    "\n",
    "if confidence >= CONFIDENCE_THRESHOLD:\n",
    "    print(f\"Diagnosis: {diagnosis}\")\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "else:\n",
    "    print(\"The model is not confident enough in its diagnosis.\")\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "    print(\"Recommendation: Consult a medical professional for further evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
